import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.datasets import boston_housing
(train_X, train_Y), (test_X, test_Y) = boston_housing.load_data()

print(type(train_X))
print(train_X.shape)

print("정규화 전",train_X[0])
x_mean = train_X.mean()
x_std = train_X.std() 
train_X = (train_X - x_mean) / x_std
test_X = (test_X - x_mean) / x_std
print("정규화 후",train_X[0])

#========전처리 완료 ==============================

print(len(train_X), len(train_Y))
print(train_X[0])
print(train_Y[0])

x_mean = train_X.mean(axis=0)
x_std = train_X.std(axis=0)
train_X -= x_mean
train_X /= x_std
test_X -= x_mean
test_X /= x_std

print(train_X[0])
print(train_X[0])

#모델 생성
model = tf.keras.Sequential([
  tf.keras.layers.Dense(units = 52, activation = 'relu', input_shape = (13,)),
  tf.keras.layers.Dense(units = 39, activation = 'relu'),
  tf.keras.layers.Dense(units = 10, activation = 'relu'),
  tf.keras.layers.Dense(units = 6, activation = 'relu'),
  tf.keras.layers.Dense(units = 1)
])
# 학습의 환경설정
# 최적화 방법 : Adam optimizer
# lr => learning late. 너무 크면 최적화 성능 떨어짐 너무 작으면 너무 학습이 오래걸림

model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.090), loss = 'mse')

model.summary()

#모델 학습
history = model.fit(train_X, train_Y, epochs = 1000, batch_size = 32, validation_split = 0.25)

#학습결과 matplotlib로 
plt.plot(history.history['loss'], 'b-', label = 'loss')
plt.plot(history.history['val_loss'], 'r--', label = 'val_loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

model.evaluate(test_X, test_Y)

pred_Y = model.predict(test_X)

plt.figure(figsize=(5,5))
plt.plot(test_Y, pred_Y, 'b.')
plt.axis([min(test_Y), max(test_Y), min(test_Y), max(test_Y)])

#y = x에 해당하는 선
plt.plot([min(test_Y), max(test_Y)], [min(test_Y), max(test_Y)], ls="--", c=".3")
plt.xlabel('test_Y')
plt.ylabel('pred_Y')

plt.show()
