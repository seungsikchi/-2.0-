#곱셈 문제 데이터 생성

X = []
Y = []
for i in range(3000):
  #0 ~ 1 범위의 랜덤한 숫자 100개를 만듭니다.
  lst = np.random.rand(100)
  #마킹할 숫자 2개의 인덱스를 뽑습니다.
  idx = np.random.choice(100, 2, replace = False)
  #마킹 인덱스가 저장된 원-핫 인코딩 백터를 만듭니다.
  zeros = np.zeros(100)
  zeros[idx] = 1
  #마킹 인덱스와 랜덤한 숫자를 합쳐서 x에 저장합니다.
  X.append(np.array(list(zip(zeros, lst))))
  Y.append(np.prod(lst[idx]))


print(X[0], Y[0])

#Simple RNN 레이어를 이용한 곱셈 문제 모델 정의
model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(units = 30, return_sequences = True, input_shape = [100,2]),
    tf.keras.layers.SimpleRNN(units = 30),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer = 'adam' , loss = 'mse')
model.summary()

# Vanishing Gradient 문제 너무 깊은 신경망에서 back propagation이 진행될수록 미분값이 0에 가까워지는 현상
# Deep RNN은 Rnn의 경우 긴문장을 해석하지 못함 그래서 이 단점을 없애고자 RNN을 Deep하게 쌓음으로써 RNN의 단점을 해소시킴

#Simple RNN 네트워크 학습
X = np.array(X)
Y = np.array(Y)

history = model.fit(X[:2560], Y[:2560], epochs = 100, validation_split = 0.2)

#Simple RNN 네트워크 학습 결과 확인
import matplotlib.pyplot as plt
plt.plot(history.history['loss'],'b-', label = 'loss')
plt.plot(history.history['val_loss'], 'r--', label = 'val_loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

#테스트 데이터에 대한 예측 정확도 확인
model.evaluate(X[2560:], Y[2560:])
prediction = model.predict(X[2560:2560+5])
for i in range(5):
  print(Y[2560+i], '\t', prediction[i][0], '\tdiff:', abs(prediction[i][0] - Y[2560+i]))

prediction = model.predict(X[2560:])
fail = 0
for i in range(len(prediction)):
  if abs(prediction[i][0] - Y[2560+i]) > 0.04:
    fail += 1

print('correctness:', (440-fail) / 440 * 100,'%')

#LSTM레이어를 이용한 곱셈 문제 모델 정의
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(units = 30, return_sequences = True, input_shape = [100,2]),
    tf.keras.layers.LSTM(units = 30),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer = 'adam', loss = 'mse')
model.summary()

# Gradient Vanishing = 안에 미분값이 1보다 작을경우 미분을 100을 하더라도 0.n으로 놀기때문에 가중치가 별로 변하지 않고 비효율적임
# Gradient Exploding = 안에 미분값이 1보다 클경우 미분을 많이 하더라도 안에 가중치가 한곳에 모이지 않고 작은값 큰값을 번걸아 가면서 나오기떄문에 트레이닝이 비효율적임
# 이 두가지 문제점을 해결하기 LSTM에서 Memory cell을 사용함 Memory cell 은 시그모이드 함수를 사용해서 전부 기억하지 않고 어느정도 비율만 기억하도록 함

#LSTM 네트워크 학습
X = np.array(X)
Y = np.array(Y)

history = model.fit(X[:2560], Y[:2560], epochs = 100, validation_split = 0.2)

#LSTM네트워크의 학습결과확인
import matplotlib.pyplot as plt
plt.plot(history.history['loss'], 'b-', label = 'loss')
plt.plot(history.history['val_loss'], 'r--', label = 'val_loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()
