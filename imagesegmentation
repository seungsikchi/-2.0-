import tensorflow_datasets as tfds
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

#데이터셋을 불러오는 명령어
dataset, info = tfds.load('oxford_iiit_pet:3.2.0', with_info = True)

#데이터셋 중에 사용할 image, segmentationmask 데이터 불러오기
train_data_len = info.splits['train'].num_examples
test_data_len = info.splits['test'].num_examples

#이미지 로드 함수
def load_image(datapoint):
  img = tf.image.resize(datapoint['image'], (128, 128))#이미지를 불러서 사이즈를 128,128로 설정
  mask = tf.image.resize(datapoint['segmentation_mask'], (128,128))#이미지 위에 segmentation할 마스크 생성 사이즈 128, 128

  img = tf.cast(img, tf.float32)#이미지를 실수형으로 변경
  img = img /255.0 #불러온 데이터를 결과가 좀더 잘 나오도록 정규화 시킴
  mask -= 1
  return img,mask

train_dataset = dataset['train'].map(load_image) #train_dataset불러오기
train_dataset = train_dataset.repeat()#데이터를 계속 학습시키기 위해 repeat()함수 사용
train_dataset = train_dataset.batch(16)#데이터의 batch_size를 16으로 설정

test_dataset = dataset['test'].map(load_image)#test_dataset불러오기
test_dataset = test_dataset.repeat()#데이터를 계속 학습시키기 위해 repeat()함수 사용
test_dataset = test_dataset.batch(1)#데이터의 batch_size를 1으로 설정

#image, segmentation mask 확인
for img, mask in train_dataset.take(1):
  plt.figure(figsize = (10,5))
  
  plt.subplot(1, 2, 1)
  plt.imshow(img[2])

  plt.subplot(1, 2, 2)
  plt.imshow(np.squeeze(mask[2], axis=2))
  plt.colorbar()
  
#REDNet 모델 생성하기(앞에서 사용한 REDNet과 동일)
def REDNet_segmentation(num_layers):
  conv_layers = []
  deconv_layers = []
  residual_layers = []

  inputs = tf.keras.layers.Input(shape=(None, None, 3))
  conv_layers.append(tf.keras.layers.Conv2D(3, kernel_size=3, padding='same', activation='relu'))

  for i in range(num_layers-1):
    conv_layers.append(tf.keras.layers.Conv2D(64, kernel_size = 3, padding ='same',activation = 'relu'))
    deconv_layers.append(tf.keras.layers.Conv2DTranspose(64, kernel_size = 3, padding = 'same',activation='relu'))
    deconv_layers.append(tf.keras.layers.Conv2DTranspose(3, kernel_size = 3, padding='same', activation='softmax'))
  x = conv_layers[0](inputs)

  #인코더 시작
  for i in range(num_layers-1):
    x = conv_layers[i+1](x)
    if i % 2 == 0:
      residual_layers.append(x)

  #디코더 시작
  for i in range(num_layers-1):
    if i % 2 == 1:
      x = tf.keras.layers.Add()([x, residual_layers.pop()])
      x = tf.keras.layers.Activation('relu')(x)
    x = deconv_layers[i](x) 

  x = deconv_layers[-1](x)
  #U-Net
import tensorflow as tf

from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Conv2DTranspose
from tensorflow.keras.layers import concatenate


def unet(n_filters=64): #flags_obj,
  # Contracting Path (encoding)
  inputs = Input(shape = (None, None, 3)) #flags_obj.input_size
  conv1 = Conv2D(n_filters, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)
  conv1 = Conv2D(n_filters, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)
  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

  conv2 = Conv2D(n_filters * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)
  conv2 = Conv2D(n_filters * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)
  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

  conv3 = Conv2D(n_filters * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)
  conv3 = Conv2D(n_filters * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)
  pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

  conv4 = Conv2D(n_filters * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)
  conv4 = Conv2D(n_filters * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)
  drop4 = Dropout(0.3)(conv4)
  pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

  conv5 = Conv2D(n_filters * 16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)
  conv5 = Conv2D(n_filters * 16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)
  drop5 = Dropout(0.3)(conv5)

  # Expansive Path (decoding)
  up6 = Conv2DTranspose(n_filters * 8, (3, 3), strides=(2, 2), padding='same')(drop5)
  merge6 = concatenate([up6, drop4], axis=3)
  conv6 = Conv2D(n_filters * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)
  conv6 = Conv2D(n_filters * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)

  up7 = Conv2DTranspose(n_filters * 4, (3, 3), strides=(2, 2), padding='same')(conv6)
  merge7 = concatenate([up7, conv3], axis=3)
  conv7 = Conv2D(n_filters * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)
  conv7 = Conv2D(n_filters * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)

  up8 = Conv2DTranspose(n_filters * 2, (3, 3), strides=(2, 2), padding='same')(conv7)
  merge8 = concatenate([up8, conv2], axis=3)
  conv8 = Conv2D(n_filters * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)
  conv8 = Conv2D(n_filters * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)

  up9 = Conv2DTranspose(n_filters, (3, 3), strides=(2, 2), padding='same')(conv8)
  merge9 = concatenate([up9, conv1], axis=3)
  conv9 = Conv2D(n_filters, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)
  conv9 = Conv2D(n_filters, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)
  conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)

  conv10 = Conv2D(3, 1, activation='sigmoid')(conv9) #sigmoid

  model = tf.keras.Model(inputs=inputs, outputs=conv10)

  return model
  
#학습하기 Epochs(학습량) = 20 
history = model.fit(train_dataset, 
                    epochs = 20, 
                    steps_per_epoch= train_data_len//16,
                    validation_data = test_dataset,
                    validation_steps = test_data_len)
                   
#분류한거 눈으로 확인하기                                   
plt.figure(figsize=(12, 12))
for idx, (img, mask) in enumerate(test_dataset.take(3)):
    plt.subplot(3, 3, idx*3+1)
    plt.imshow(img[0])
    
    plt.subplot(3, 3, idx*3+2)
    plt.imshow(np.squeeze(mask[0], axis=2))
    
    predict = tf.argmax(model.predict(img), axis=-1)
    plt.subplot(3, 3, idx*3+3)
    # plt.imshow(np.squeeze(predict[0], axis=0))
    plt.imshow(np.squeeze(predict, axis=0))
    
#테스트 이미지 분할 확인(원본)
plt.figure(figsize=(12, 12))
for idx, datapoint in enumerate(dataset['test'].take(3)):
    img = datapoint['image']
    mask = datapoint['segmentation_mask']
    
    img = tf.cast(img, tf.float32)
    img = img / 255.0
    mask -= 1
    
    plt.subplot(3, 3, idx*3+1)
    plt.imshow(img)
    
    plt.subplot(3, 3, idx*3+2)
    plt.imshow(np.squeeze(mask, axis=2))
    
    predict = tf.argmax(model.predict(tf.expand_dims(img, axis=0)), axis=-1)
    plt.subplot(3, 3, idx*3+3)
    # plt.imshow(np.squeeze(predict[0], axis=0))
    plt.imshow(np.squeeze(predict, axis=0))
  
